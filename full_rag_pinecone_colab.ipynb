{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd0e Full RAG Chatbot with Pinecone + LangChain (Google Colab)\n",
        "\n",
        "This notebook implements:\n",
        "- URL ingestion\n",
        "- Text chunking\n",
        "- HuggingFace embeddings\n",
        "- Pinecone vector database\n",
        "- Retrieval-Augmented Generation (RAG)\n",
        "- Local open-source LLM (no OpenAI)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip -q install pinecone-client langchain-community sentence-transformers transformers accelerate unstructured nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# \ud83d\udd10 PASTE YOUR PINECONE API KEY HERE\n",
        "os.environ['PINECONE_API_KEY'] = 'PASTE_YOUR_API_KEY_HERE'\n",
        "\n",
        "# \ud83d\udd10 PASTE YOUR INDEX HOST HERE (from Pinecone dashboard)\n",
        "INDEX_NAME = 'lamaproject'\n",
        "INDEX_HOST = 'PASTE_INDEX_HOST_HERE'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
        "index = pc.Index(name=INDEX_NAME, host=INDEX_HOST)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Pinecone as PineconeVectorStore\n",
        "from transformers import pipeline\n",
        "import uuid\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "URLS = [\n",
        "    'https://blog.openai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models/',\n",
        "    'https://www.mosaicml.com/blog/mpt-7b',\n",
        "    'https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models'\n",
        "]\n",
        "\n",
        "loader = UnstructuredURLLoader(urls=URLS)\n",
        "documents = loader.load()\n",
        "print('Loaded documents:', len(documents))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "text_splitter = CharacterTextSplitter(\n",
        "    separator='\\n',\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "text_chunks = text_splitter.split_documents(documents)\n",
        "print('Total chunks:', len(text_chunks))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name='sentence-transformers/all-mpnet-base-v2',\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "texts = [doc.page_content for doc in text_chunks]\n",
        "vectors = embeddings.embed_documents(texts)\n",
        "\n",
        "upserts = [\n",
        "    (str(uuid.uuid4()), vector, {'text': text})\n",
        "    for vector, text in zip(vectors, texts)\n",
        "]\n",
        "\n",
        "index.upsert(vectors=upserts)\n",
        "print('\u2705 Data successfully stored in Pinecone')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vectorstore = PineconeVectorStore(\n",
        "    index=index,\n",
        "    embedding=embeddings,\n",
        "    text_key='text'\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "generator = pipeline(\n",
        "    'text-generation',\n",
        "    model='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
        "    max_new_tokens=300,\n",
        "    temperature=0.3\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def ask_question(query, k=3):\n",
        "    docs = vectorstore.similarity_search(query, k=k)\n",
        "    context = '\\n\\n'.join([d.page_content for d in docs])\n",
        "    prompt = f'''You are a helpful AI assistant.\n",
        "Use the context below to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer:\n",
        "'''\n",
        "    return generator(prompt)[0]['generated_text']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('\ud83e\udd16 Chatbot ready! Type exit to stop.')\n",
        "while True:\n",
        "    q = input('You: ')\n",
        "    if q.lower() == 'exit':\n",
        "        break\n",
        "    print('\\nBot:', ask_question(q), '\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}